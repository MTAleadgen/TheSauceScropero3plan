#!/usr/bin/env python3
"""
Test script for DataForSEO API with simple query format
Trying to match the approach that worked with SerpAPI
"""
import os
import json
import base64
import requests
import logging
import time
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables
load_dotenv(override=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Configuration
DATAFORSEO_LOGIN = os.getenv("DATAFORSEO_LOGIN")
DATAFORSEO_PASSWORD = os.getenv("DATAFORSEO_PASSWORD")
DATA_RAW_DIR = "./data_raw"

# Ensure data_raw directory exists
Path(DATA_RAW_DIR).mkdir(parents=True, exist_ok=True)

def test_simple_query():
    """Test with a very simple query format"""
    if not DATAFORSEO_LOGIN or not DATAFORSEO_PASSWORD:
        logger.error("DataForSEO credentials not set.")
        return False
    
    # DataForSEO API endpoint for Google Organic Search
    endpoint = "https://api.dataforseo.com/v3/serp/google/organic/live/advanced"
    
    # Create auth header
    auth_header = base64.b64encode(f"{DATAFORSEO_LOGIN}:{DATAFORSEO_PASSWORD}".encode()).decode()
    headers = {
        'Authorization': f'Basic {auth_header}',
        'Content-Type': 'application/json'
    }
    
    # Create the search task with simple query
    # This matches what worked with SerpAPI
    data = [{
        "keyword": "salsa in new york",
        "location_name": "United States",  # Just using country to avoid over-specification
        "language_name": "English",
        "depth": 1,  # Changed from 3 to 1
        "se_domain": "google.com"
    }]
    
    try:
        # Make the API request
        logger.info(f"Making API request with simple query: 'salsa in new york'")
        response = requests.post(endpoint, json=data, headers=headers)
        response.raise_for_status()
        result = response.json()
        
        # Save the entire response
        raw_path = os.path.join(DATA_RAW_DIR, "test_simple_query_response.json")
        with open(raw_path, 'w') as f:
            json.dump(result, f, indent=2)
        logger.info(f"Saved raw API response to {raw_path}")
        
        # Log response status code
        logger.info(f"DataForSEO API status code: {result.get('status_code')} - {result.get('status_message')}")
        
        # Process results if available
        if result.get('status_code') == 20000:  # Success code
            tasks = result.get('tasks', [])
            
            total_urls = 0
            organic_count = 0
            
            # Count URLs from all result types, not just organic
            for task in tasks:
                task_id = task.get('id')
                logger.info(f"Processing task ID: {task_id}")
                
                if 'result' in task and task['result']:
                    for i, item in enumerate(task['result']):
                        # Save each result item separately
                        result_path = os.path.join(DATA_RAW_DIR, f"test_simple_query_{i}.json")
                        with open(result_path, 'w') as f:
                            json.dump(item, f, indent=2)
                        
                        # Extract ALL URLs from different types of results
                        items = item.get('items', [])
                        
                        page_urls = []
                        for result_item in items:
                            item_type = result_item.get('type')
                            
                            if item_type == 'organic':
                                url = result_item.get('url')
                                if url:
                                    page_urls.append(f"[organic] {url}")
                                    organic_count += 1
                            
                            elif item_type == 'local_pack':
                                url = result_item.get('url')
                                if url:
                                    page_urls.append(f"[local_pack] {url}")
                                
                                # Also check for domain
                                domain = result_item.get('domain')
                                if domain:
                                    domain_url = f"https://{domain}"
                                    page_urls.append(f"[local_pack_domain] {domain_url}")
                            
                            elif item_type == 'video':
                                # Extract URLs from video items
                                video_items = result_item.get('items', [])
                                for video_item in video_items:
                                    url = video_item.get('url')
                                    if url:
                                        page_urls.append(f"[video] {url}")
                            
                            # Look for links inside results
                            links = result_item.get('links', [])
                            if links:
                                for link in links:
                                    link_url = link.get('url')
                                    if link_url:
                                        page_urls.append(f"[link] {link_url}")
                        
                        # Log all URLs found on this page
                        logger.info(f"Found {len(page_urls)} URLs on result page {i}")
                        for url in page_urls:
                            logger.info(f"  {url}")
                        
                        total_urls += len(page_urls)
                else:
                    logger.warning(f"No results found in task {task_id}")
            
            logger.info(f"Total URLs found: {total_urls} (including {organic_count} organic results)")
        else:
            logger.error(f"API request failed: {result.get('status_message')}")
            return False
        
        return True
    
    except requests.exceptions.RequestException as e:
        logger.error(f"Request failed: {e}")
        return False
    
    except Exception as e:
        logger.error(f"An unexpected error occurred: {e}")
        return False

if __name__ == "__main__":
    logger.info("Starting DataForSEO API test with simple query format")
    
    if test_simple_query():
        logger.info("DataForSEO API test completed successfully")
    else:
        logger.error("DataForSEO API test failed") 